# Policy Gap Analyzer (Offline, Local LLM)

A fully **offline-capable policy gap analysis tool** that compares organizational policies against reference frameworks (CIS, MS-ISAC, NIST, etc.) using **local embeddings, FAISS similarity search, and a local LLM via Ollama**.

This project is designed for **security audits, compliance testing, and LLM evaluation** without relying on cloud APIs.

---

## ğŸš€ Key Features

- âœ… 100% **offline runtime** (after initial setup)
- âœ… Local **LLM inference via Ollama**
- âœ… **SentenceTransformer embeddings**
- âœ… **FAISS** for fast semantic similarity search
- âœ… Supports **TXT / DOCX / PDF**
- âœ… Batch comparison of multiple policies
- âœ… Deterministic, testable gap detection
- âœ… Designed for **Intel CPU (no GPU required)**

---

## ğŸ§  Architecture Overview

```
Reference Policies â”€â”€â”
                     â”œâ”€â–¶ SentenceTransformer â”€â–¶ FAISS Index
Test Policies â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
                                                     â–¼
                                           Gap Candidates
                                                     â”‚
                                                     â–¼
                                           Local LLM (Ollama)
                                                     â”‚
                                                     â–¼
                                            Gap Report Files
```

---

## ğŸ“¦ Requirements

### System
- Ubuntu 20.04+ / WSL2
- Python **3.9 â€“ 3.12**
- 16 GB RAM recommended
- Intel CPU supported (no CUDA needed)

---

## ğŸ”§ One-Time Online Setup (Required)

### 1ï¸âƒ£ System packages
```bash
sudo apt update
sudo apt install -y   python3 python3-pip python3-venv   build-essential   poppler-utils   git curl
```

---

### 2ï¸âƒ£ Install Ollama (Local LLM Runtime)
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Verify:
```bash
ollama --version
```

---

### 3ï¸âƒ£ Download LLM Model (DO THIS ONCE)
```bash
ollama pull llama3.2:3b
```

---

### 4ï¸âƒ£ Start Ollama Server
```bash
ollama serve
```

---

## ğŸ Python Environment Setup

```bash
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt --break-system-packages
```

---

## ğŸ—‚ Data & Model Caching

```bash
export TRANSFORMERS_CACHE=$PWD/cache/transformers
export HF_HOME=$PWD/cache/huggingface
export FAISS_CACHE_PATH=$PWD/cache/faiss
export OLLAMA_MODELS=$HOME/.ollama/models
```

---

## â–¶ï¸ Running the Analyzer

```bash
python3 policy_gap_analyzer.py   --reference_folder ./refs   --test_folder ./tests   --output ./reports
```

---

## â± Performance (Intel i5, 16GB RAM)

- Embeddings: 2â€“4 minutes
- FAISS indexing: < 30 seconds
- LLM analysis: 3â€“8 minutes

---

## ğŸ” Offline Guarantee

| Component | Offline |
|---------|---------|
| Python code | âœ… |
| FAISS | âœ… |
| SentenceTransformer | âœ… |
| Ollama inference | âœ… |
| Internet APIs | âŒ |

---

